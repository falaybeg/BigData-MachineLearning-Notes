{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Machine Learning Clustering using K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Segmentation is used to divide a customer base into groups of individuals that are similar in a specific way relevant to marketing, such as age, gender, interest... etc. This project enables us to cluster our customers for a growing company's revenue. \n",
    "Project's customer dataset includes Gender, Age, Annual Income and Spending Score features, and it does not include label feature. Therefore, clustering is the most suitable method where doesn't have a label feature in the dataset. As a result, in this project we perform a customer segmentation we apply the K-Means algorithm which is one of the most popular clustering algorithms. \n",
    "\n",
    "\n",
    "QUESTION: How many clusters do I need to divide the customers into ?\n",
    "\n",
    "According to above question bussiness decision will be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "pyspark = SparkSession.builder \\\n",
    ".master(\"local[4]\")\\\n",
    ".appName(\"KMeansClusterin\")\\\n",
    ".config(\"spark.executer.memory\",\"3g\")\\\n",
    ".config(\"spark.driver.memory\",\"3g\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "sc = pyspark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"True\")\\\n",
    ".option(\"inferSchema\", \"True\")\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".load(\"Mall-Customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------+-------------+\n",
      "|CustomerID|Gender|Age|AnnualIncome|SpendingScore|\n",
      "+----------+------+---+------------+-------------+\n",
      "|         1|  Male| 19|       15000|           39|\n",
      "|         2|  Male| 21|       15000|           81|\n",
      "|         3|Female| 20|       16000|            6|\n",
      "|         4|Female| 23|       16000|           77|\n",
      "|         5|Female| 31|       17000|           40|\n",
      "+----------+------+---+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Dropping the CustomerID \n",
    "\n",
    "We don't use CustomerID in clustering. Hence, CustomerID is deleted from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------+-------------+\n",
      "|Gender|Age|AnnualIncome|SpendingScore|\n",
      "+------+---+------------+-------------+\n",
      "|  Male| 19|       15000|           39|\n",
      "|  Male| 21|       15000|           81|\n",
      "|Female| 20|       16000|            6|\n",
      "+------+---+------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df2 = customer_df.drop(\"CustomerID\")\n",
    "customer_df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Checking NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 . Gender --> \u001b[1;32;1m is clean \u001b[0m\n",
      "2 . Age --> \u001b[1;32;1m is clean \u001b[0m\n",
      "3 . AnnualIncome --> \u001b[1;32;1m is clean \u001b[0m\n",
      "4 . SpendingScore --> \u001b[1;32;1m is clean \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "count_for_null = 1\n",
    "for column in customer_df2.columns:\n",
    "    if(customer_df2.filter(col(column).isNull()).count()>0):\n",
    "        print(count_for_null, \".\", column, \"--> \\033[1;31;1m there has null values \\033[0m\")\n",
    "    else:\n",
    "        print(count_for_null, \".\",column,\"--> \\033[1;32;1m is clean \\033[0m\")\n",
    "    count_for_null += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Checking the gender text character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does all gender name start with uppercase ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Gender|count|\n",
      "+------+-----+\n",
      "|Female|  112|\n",
      "|  Male|   88|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df2.groupBy(\"Gender\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Checking max and min ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have any outlier value in Age, AnnualIncome and SpedingScore ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+\n",
      "|summary|              Age|      AnnualIncome|     SpendingScore|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "|  count|              200|               200|               200|\n",
      "|   mean|            38.85|           60560.0|              50.2|\n",
      "| stddev|13.96900733155888|26264.721165271247|25.823521668370173|\n",
      "|    min|               18|             15000|                 1|\n",
      "|    max|               70|            137000|                99|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df2.select(\"Age\",\"AnnualIncome\",\"SpendingScore\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation (Transform) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. StringIndexer Process (Categorical Features)\n",
    "\n",
    "\n",
    "Categories (A, B, C) and after StringIndexer --> Categories(0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------+-------------+------------+\n",
      "|Gender|Age|AnnualIncome|SpendingScore|Gender_Index|\n",
      "+------+---+------------+-------------+------------+\n",
      "|  Male| 19|       15000|           39|         1.0|\n",
      "|  Male| 21|       15000|           81|         1.0|\n",
      "|Female| 20|       16000|            6|         0.0|\n",
      "+------+---+------------+-------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gender_index = StringIndexer()\\\n",
    ".setInputCol(\"Gender\")\\\n",
    ".setOutputCol(\"Gender_Index\")\n",
    "\n",
    "gender_index_model = gender_index.fit(customer_df2)\n",
    "gender_index_df = gender_index_model.transform(customer_df2)\n",
    "gender_index_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. OneHotEncoderEstimator Process (Categorical Features)\n",
    "\n",
    "CategoryIndex(0,0,1,0,0) --> This label is 3th index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------+-------------+------------+--------------+\n",
      "|Gender|Age|AnnualIncome|SpendingScore|Gender_Index|Gender_Encoded|\n",
      "+------+---+------------+-------------+------------+--------------+\n",
      "|  Male| 19|       15000|           39|         1.0|     (1,[],[])|\n",
      "|  Male| 21|       15000|           81|         1.0|     (1,[],[])|\n",
      "|Female| 20|       16000|            6|         0.0| (1,[0],[1.0])|\n",
      "+------+---+------------+-------------+------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoderEstimator()\\\n",
    ".setInputCols([\"Gender_Index\"])\\\n",
    ".setOutputCols([\"Gender_Encoded\"])\n",
    "\n",
    "encoder_model = encoder.fit(gender_index_df)\n",
    "encoder_df = encoder_model.transform(gender_index_df)\n",
    "encoder_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. VectorAssembler  (Transforming features into vector)\n",
    "\n",
    "All input values should be in a single feature for Machine Learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler()\\\n",
    ".setInputCols([\"Gender_Encoded\",\"Age\",\"AnnualIncome\",\"SpendingScore\"])\\\n",
    ".setOutputCol(\"vectorized_features\")\n",
    "\n",
    "assembler_df = assembler.transform(encoder_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. Normalization\n",
    "\n",
    "\n",
    "StandardScaler transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  4.4.1 Using StandardScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------+-------------+------------+--------------+--------------------+--------------------+\n",
      "|Gender|Age|AnnualIncome|SpendingScore|Gender_Index|Gender_Encoded| vectorized_features|            features|\n",
      "+------+---+------------+-------------+------------+--------------+--------------------+--------------------+\n",
      "|  Male| 19|       15000|           39|         1.0|     (1,[],[])|[0.0,19.0,15000.0...|[0.0,1.3601539142...|\n",
      "|  Male| 21|       15000|           81|         1.0|     (1,[],[])|[0.0,21.0,15000.0...|[0.0,1.5033280104...|\n",
      "|Female| 20|       16000|            6|         0.0| (1,[0],[1.0])|[1.0,20.0,16000.0...|[2.00951470525829...|\n",
      "+------+---+------------+-------------+------------+--------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\\\n",
    ".setInputCol(\"vectorized_features\")\\\n",
    ".setOutputCol(\"features\")\\\n",
    "\n",
    "standard_scale_model = standard_scaler.fit(assembler_df)\n",
    "standard_df = standard_scale_model.transform(assembler_df)\n",
    "standard_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.2 Using Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------+-------------+------------+--------------+--------------------+--------------------+\n",
      "|Gender|Age|AnnualIncome|SpendingScore|Gender_Index|Gender_Encoded| vectorized_features|            features|\n",
      "+------+---+------------+-------------+------------+--------------+--------------------+--------------------+\n",
      "|  Male| 19|       15000|           39|         1.0|     (1,[],[])|[0.0,19.0,15000.0...|[0.0,0.0012666613...|\n",
      "|  Male| 21|       15000|           81|         1.0|     (1,[],[])|[0.0,21.0,15000.0...|[0.0,0.0013999782...|\n",
      "|Female| 20|       16000|            6|         0.0| (1,[0],[1.0])|[1.0,20.0,16000.0...|[6.24999466553417...|\n",
      "+------+---+------------+-------------+------------+--------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer()\\\n",
    ".setInputCol(\"vectorized_features\")\\\n",
    ".setOutputCol(\"features\")\\\n",
    "\n",
    "normal_df = normalizer.transform(assembler_df)\n",
    "normal_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = normal_df.randomSplit([0.8, 0.2], seed=142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = standard_df.randomSplit([0.8, 0.2], seed=142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spark ML (Using Pipeline) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Applying K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>AnnualIncome</th>\n",
       "      <th>SpendingScore</th>\n",
       "      <th>features</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>19</td>\n",
       "      <td>65000</td>\n",
       "      <td>50</td>\n",
       "      <td>[2.0095147052582996, 1.360153914235199, 2.4748...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>30000</td>\n",
       "      <td>73</td>\n",
       "      <td>[2.0095147052582996, 1.503328010470483, 1.1422...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>62000</td>\n",
       "      <td>42</td>\n",
       "      <td>[2.0095147052582996, 1.503328010470483, 2.3605...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>22</td>\n",
       "      <td>17000</td>\n",
       "      <td>76</td>\n",
       "      <td>[2.0095147052582996, 1.574915058588125, 0.6472...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>70000</td>\n",
       "      <td>29</td>\n",
       "      <td>[2.0095147052582996, 1.6465021067057672, 2.665...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  AnnualIncome  SpendingScore  \\\n",
       "0  Female   19         65000             50   \n",
       "1  Female   21         30000             73   \n",
       "2  Female   21         62000             42   \n",
       "3  Female   22         17000             76   \n",
       "4  Female   23         70000             29   \n",
       "\n",
       "                                            features  cluster  \n",
       "0  [2.0095147052582996, 1.360153914235199, 2.4748...        2  \n",
       "1  [2.0095147052582996, 1.503328010470483, 1.1422...        0  \n",
       "2  [2.0095147052582996, 1.503328010470483, 2.3605...        2  \n",
       "3  [2.0095147052582996, 1.574915058588125, 0.6472...        0  \n",
       "4  [2.0095147052582996, 1.6465021067057672, 2.665...        2  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means = KMeans()\\\n",
    ".setFeaturesCol(\"vectorized_features\")\\\n",
    ".setPredictionCol(\"cluster\")\\\n",
    ".setK(3)\n",
    "\n",
    "kmeans_model = k_means.fit(train_df)\n",
    "result_df = kmeans_model.transform(test_df)\n",
    "\n",
    "result_df.select(\"Gender\",\"Age\",\"AnnualIncome\",\"SpendingScore\",\"features\",\"cluster\").toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|cluster|count|\n",
      "+-------+-----+\n",
      "|      1|    4|\n",
      "|      2|   20|\n",
      "|      0|   19|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.groupBy(\"cluster\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Method 1 (using single K value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score (for k=3):  0.13655564392757646\n"
     ]
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator()\\\n",
    ".setFeaturesCol(\"features\")\\\n",
    ".setPredictionCol(\"cluster\")\\\n",
    ".setMetricName(\"silhouette\")\n",
    "\n",
    "score = evaluator.evaluate(result_df)\n",
    "print(\"Silhouette Score (for k=3): \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Method 2 (using for loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestKmeans(kValue):\n",
    "    for i in range(1,kValue):\n",
    "        k_means = KMeans()\\\n",
    "        .setFeaturesCol(\"vectorized_features\")\\\n",
    "        .setPredictionCol(\"cluster\")\\\n",
    "        .setK(i+1)\n",
    "        kmeans_model = k_means.fit(train_df)\n",
    "        result_df = kmeans_model.transform(test_df)\n",
    "\n",
    "        evaluator = ClusteringEvaluator()\\\n",
    "        .setFeaturesCol(\"features\")\\\n",
    "        .setPredictionCol(\"cluster\")\\\n",
    "        .setMetricName(\"silhouette\")\n",
    "\n",
    "        score = evaluator.evaluate(result_df)\n",
    "        print(\"Silhouette Score (for k=\",i+1, \"): \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "We can say that k=6 gives the best score compared to other k values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score (for k= 2 ):  0.25460377577865234\n",
      "Silhouette Score (for k= 3 ):  0.13655564392757646\n",
      "Silhouette Score (for k= 4 ):  0.07646596820179498\n",
      "Silhouette Score (for k= 5 ):  0.05283117853305441\n",
      "Silhouette Score (for k= 6 ):  0.07634479551635422\n",
      "Silhouette Score (for k= 7 ):  0.0025455579209068243\n",
      "Silhouette Score (for k= 8 ):  -0.12702671178000344\n",
      "Silhouette Score (for k= 9 ):  -0.15919934575144346\n",
      "Silhouette Score (for k= 10 ):  -0.21882903110324425\n"
     ]
    }
   ],
   "source": [
    "maxKValue = 10\n",
    "bestKmeans(maxKValue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
